{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste Data Science Elo7 - Descrição do Processo\n",
    "\n",
    "Esse notebook descreve o processo de implementação do sistema de recomendação de filmes a partir da base Movie Lens como requisito do processo seletivo do Elo7.\n",
    "\n",
    "Segundo recomendação do processo, esse notebook vai descrever a estratégia e lógica da solução proposta. O documento está dividido em 4 partes: Estrutura do Projeto, Análise Exploratória, Sistema de Recomendação e Avaliação do Sistema de Recomendação\n",
    "\n",
    "O documento pode alterar de acordo com a evolução da solução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Estrutura do Projeto\n",
    "\n",
    "Um projeto de Data Science pode seguir diversas estruturas e padrões diferentes. Apesar dessa liberdade dar flexibilidade para o cientista de dados, ela dificulta a reprodução do experimento, necessitando de um entendimento do projeto antes de executá-lo. Para suavizar essa dificultade, este projeto vai seguir o template <a href='https://drivendata.github.io' target='blank'>Cookiecutter Data Science</a>.\n",
    "\n",
    "O objetivo deste template é organizar um projeto de Data Science, separando as responsabilidades em diretórios diferentes. Por exemplo, ao invés de produzir e reproduzir todo o código nos notebooks (que dificulta a leitura e incentiva más práticas de programação), o template apresenta um diretório de código fonte separando funções de dados, modelos, visualizações e atributos.\n",
    "\n",
    "A estrutura do projeto é definda a seguir:\n",
    "\n",
    "\n",
    "```bash\n",
    "├── LICENSE\n",
    "    ├── Makefile           <- Makefile with commands like `make data` or `make train`\n",
    "    ├── README.md          <- The top-level README for developers using this project.\n",
    "    ├── data\n",
    "    │   ├── external       <- Data from third party sources.\n",
    "    │   ├── interim        <- Intermediate data that has been transformed.\n",
    "    │   ├── processed      <- The final, canonical data sets for modeling.\n",
    "    │   └── raw            <- The original, immutable data dump.\n",
    "    │\n",
    "    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details\n",
    "    │\n",
    "    ├── models             <- Trained and serialized models, model predictions, or model summaries\n",
    "    │\n",
    "    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "    │                         the creators initials, and a short \n",
    "    │\n",
    "    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n",
    "    │\n",
    "    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "    │   └── figures        <- Generated graphics and figures to be used in reporting\n",
    "    │\n",
    "    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n",
    "    │                         generated with `pip freeze > requirements.txt`\n",
    "    │\n",
    "    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n",
    "    ├── src                <- Source code for use in this project.\n",
    "    │   ├── __init__.py    <- Makes src a Python module\n",
    "    │   │\n",
    "    │   ├── data           <- Scripts to download or generate data\n",
    "    │   │   └── make_dataset.py\n",
    "    │   │\n",
    "    │   ├── features       <- Scripts to turn raw data into features for modeling\n",
    "    │   │   └── build_features.py\n",
    "    │   │\n",
    "    │   ├── models         <- Scripts to train models and then use trained models to make\n",
    "    │   │   │                 predictions\n",
    "    │   │   ├── predict_model.py\n",
    "    │   │   └── train_model.py\n",
    "    │   │\n",
    "    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations\n",
    "    │       └── visualize.py\n",
    "    │\n",
    "    └── tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n",
    "```\n",
    "\n",
    "Para fins de simplificação, a estrutura do presente projeto será resumida da seguinte forma:\n",
    "\n",
    "```bash\n",
    "├── LICENSE\n",
    "    ├── README.md          <- The top-level README for developers using this project.\n",
    "    ├── data\n",
    "    │   ├── processed      <- The final, canonical data sets for modeling.\n",
    "    │   └── raw            <- The original, immutable data dump.\n",
    "    │\n",
    "    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "    │                         the creators initials, and a short \n",
    "    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n",
    "    │                         generated with `pip freeze > requirements.txt`\n",
    "    │\n",
    "    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n",
    "    ├── src                <- Source code for use in this project.\n",
    "    │   ├── __init__.py    <- Makes src a Python module\n",
    "    │   │\n",
    "    │   ├── data           <- Scripts to download or generate data\n",
    "    │   │   └── make_dataset.py\n",
    "    │   │\n",
    "    │   ├── features       <- Scripts to turn raw data into features for modeling\n",
    "    │   │   └── build_features.py\n",
    "    │   │\n",
    "    │   ├── models         <- Scripts to train models and then use trained models to make\n",
    "    │   │   │                 predictions\n",
    "    │   │   |\n",
    "    │   │   └── predict_model.py\n",
    "    │   │\n",
    "    │   └── visualization  <- Scripts to create exploratory and results oriented visualizations\n",
    "    │       └── visualize.py\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "Por convenção, as funções e variáveis do projeto serão escritas na língua inglesa, enquanto as explicações e texto serão escritos na língua portuguesa.\n",
    "\n",
    "O projeto será desenvolvido em Python, utilizando bibliotecas comuns em projetos de Data Science (Pandas, Numpy, Scikit-learn, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Análise Exploratória e Pré-processamento\n",
    "\n",
    "A análise exploratória de dados é, usualmente, a primeira parte de todo projeto de Data Science. Ela serve para conhecer os dados, saber do que se trata, como são organizados e estruturados, etc.\n",
    "\n",
    "Nessa parte inicial todos os arquivos serão carregados em DataFrames, observando algumas características como quantidade de registros, tipo das variáveis, número de variáveis nulas, quantidade de valores únicos, quartis de variáveis numéricas, etc. É nessa parte que as hipóteses sobre os dados são definidas e avaliadas (dependendo da complexidade da hipótese). Ao analisar os dados, alguns pré-processamentos serão calculados para facilitar análises futuras.\n",
    "\n",
    "Dado à limitação de hardware, a base inicial de 20 milhões de avaliações foi substituída pela base com 1 milhão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Sistema de Recomendação\n",
    "\n",
    "As técnicas para sistema de recomendação podem ser divididas em dois grupos: baseadas em memória ou baseadas em modelos. As técnicas baseadas em memória recorrem aos items consumidos pelos usuários para gerar novas recomendações. A famosa collaborative filtering (user-based ou item-based) é um exemplo de técnica baseada em memória. \n",
    "\n",
    "Já as técnicas baseadas em modelo treinam modelos que ajudam a prever, por exemplo, a nota que um usuário daria para um item (através de modelos de regressão). Ou qual a probabilidade do usuário gostar ou não de um item (através de modelos de classificação).\n",
    "\n",
    "Existem algumas bibliotecas python que implementam algumas técnicas de sistemas de recomendação como a <a href='http://surpriselib.com/' target='blank'>Suprise</a> ou a <a href='https://github.com/caserec/CaseRecommender' target='blank'>Case Recommender</a>. Existem casos onde seria possível e até recomendado utilizar essas bibliotecas, poupando tempo e esforço do time no desenvolvimento de um sistema de recomendação. Mas, ao construir seu próprio sistema de recomendação, é possível personalizá-lo de acordo a preferência e necessidade da aplicação.\n",
    "\n",
    "Com a base de dados fornecida temo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Sistema de Recomendação\n",
    "\n",
    "Sistemas de recomendação é uma forma de filtrar itens e produtos de modo a maximizar e melhorar o consumo do usuário. Em tempos que empresas oferecem diversas opções de produtos, um sistema que apresenta as melhores opções personalizada para cada usuário otimiza o processo, deixando o usuário mais feliz com o produto e aumentando a receita da empresa, com uma venda acertiva. Empresas como Netflix, Amazon, Google, portais de notícias, Youtube, Facebook e etc utilizam sistemas de recomendação em alguma parte de seus produtos.\n",
    "\n",
    "Exitem várias técnicas para se implementar um sistemas de recomendação. A escolha de uma técnica pode variar de acordo com os dados fornecidos, a velocidade necessária de resposta ou até o método de avaliação dos usuários (iteração explícita como nota ou implícita como compra). Entre as técnicas pode-se destacar a recomendação direta de itens (seguindo alguma estratégia) ou de predição de nota (através de algoritmos de regressão).\n",
    "\n",
    "É possível, por exemplo, recomendar o item mais comum ou mais bem avaliado, recomendar um item genérico, usar a técnica de itens frequentes apriori ou usar técnicas de filtro colaborativo (Collaborative Filtering), que baseia na iteração e consumo dos usuários para recomendar um outro item. Técnicas de filtro colaborativo podem ser divididas em duas classes: as baseadas em memória e baseada em modelo. Baseada em memória utiliza técnicas de similaridade (distância euclideana ou coseno, por exemplo) entre vetores para identificar usuários ou itens similares e, a partir disso, gerar recomendações. Ela funciona bem para bases pequenas e quando não temos matrizes exparsas. \n",
    "\n",
    "Já técnicas baseadas em modelos utilizam o histórico de consumo ou avaliação dos itens para treinar uma máquina de predição para recomendar itens para o usuário . É possível usar técnicas de regressão para prever o valor de uma nota ou técnicas de classificação para prever a probabilidade do consumo do item. Dependendo do contexto dos dados é possível usar técnicas diferentes.\n",
    "\n",
    "Existem algumas bibliotecas python que implementam algumas técnicas de sistemas de recomendação como a <a href='http://surpriselib.com/' target='blank'>Suprise</a> ou a <a href='https://github.com/caserec/CaseRecommender' target='blank'>Case Recommender</a>. Ambas contam com uma grande gama de algoritmos e métodos de sistemas de recomendação.\n",
    "\n",
    "Na base de dados trabalhada nesse teste, temos as seguintes informações:\n",
    "\n",
    "- Filme: nome, ano, lista de gêneros\n",
    "- Usuário: faixa-etaria, profissão, localidade\n",
    "- Avaliação: valor da nota (escala 1 a 5), data e hora da avaliação\n",
    "- (extra) Links: descrição do filme\n",
    "\n",
    "Com esses dados vamos testar três abordagens: recomendação dos top filmes, filtro colaborativo (baseado em usuário) e fatorização de matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Avaliação\n",
    "\n",
    "A avaliação será feita em três par"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
